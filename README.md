# Inter-IIT-DevRev

## Problem Statement - Key Points

- Paragraph Retrieval : Mapping user queries to the most relevant context in the Knowledge base.
- Predicting if query is answerable or not. Span Prediction if answerable.
- Answer Retrieval : From the retrieved context
- Minimising Latency and creating space efficient models

## Our Complete Model

<img width="607" alt="image" src="https://github.com/CodingWarrior33/Inter-IIT-DevRev/assets/95586108/8799c918-1b90-4d0c-914a-925cb0f51ff4">


## Pipeline Structure 

<img width="1304" alt="image" src="https://github.com/CodingWarrior33/Inter-IIT-DevRev/assets/95586108/2e6218da-c82e-4e36-a4c8-edbe6241aa47">

## Synthetic Data Generation

The pipeline for generating synthetic data focuses on Data Augmentation techniques which is more suited that traditional GANs for fast paced Question Answer Generation. 

- Average Generation Time per Question Answer Pair: 2s

- Quality of Generated Question - Answer Pairs:

	- F1 score of generated question-answer pairs = 0.80855  
	- This F1 score was generated by comparing generated answers with answers given by large Question/Answering Models.

<img width="1303" alt="image" src="https://github.com/CodingWarrior33/Inter-IIT-DevRev/assets/95586108/23b908a0-7505-4232-8608-5d7576688ceb">


## Answer Retrieval 

- Sketchy Reading : Makes an initial Judgement about answerability of a question. Three main subprocesses.
	-  Embedding generation  
	-  Interaction  
	-  External Front Verification
- Intensive Reading : Verifies answerability of earlier predictions through application of Multi-headed cross attention and threshold verification
- Rear Verification: Score combination of results of both the modules - Sketchy and Intensive.

  <img width="548" alt="image" src="https://github.com/CodingWarrior33/Inter-IIT-DevRev/assets/95586108/3be024be-2b54-4332-beae-58a6711aac6e">

## Main Unit for Inference : Retro-Reader
<img width="1382" alt="image" src="https://github.com/CodingWarrior33/Inter-IIT-DevRev/assets/95586108/1e7fb9e6-be3c-46c7-9ded-6c5270c0858f">


## Other Features
- Naive Implementation of Deformer: Deformer architecture was implemented naively by effectively changing last layers of a model to work with a different architecture. Thus a Roberta Model working on an Electra Architecture serves for a simple deformer layout.
- Transformer Compression: Quantisation, Prunification.


**Team Members :**
- Samvaidan (Team Leader)
- Akarshan 
- Taraksh 
- Ekansh 
- Vansh
- Arush 
- Ashutosh
- Mukesh 
- Deepali
- Raj Singh

